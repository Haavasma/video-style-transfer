\subsection{Artistic style transfer for videos}
\label{seq:ruder}
\textit{Artistic style transfer for videos} is a paper written by Manuel Ruder et al. \cite{Ruder:1} And is based on Gatys \cite{Gatys:1} way of implementing style transfer, but introduces a temporal constraint in order to get rid of the randomness that appears when styling each frame individually. \nextline
\subsubsection{Loss function}
The temporal constraint is described in the loss function of the style transfer model. The idea is to penalize the model for deviations between two adjacent frames. So as well as having a content-loss and a style-loss, they introduce a third component for temporal loss: 
\begin{equation}
\mathcal{L}_{temporal}(x, \omega, c) = \frac{1}{D}\sum_{k=1} c_k \cdot (x_k - \omega_k)^2
\end{equation}
\nextline This loss component takes three arguments: x, $\omega$ and c. x is the model variable, or more specifically the image we are styling. $\omega$ is the warped previous styled frame. By warped we mean warped using the flow calculated between the current frame and the previous frame. This can be done using a variety of flow calculation algorithms and the article mentions some of these, including DeepFlow and EpicFlow. The c parameter is a mask with the same shape as the styled image, containing integer values 0 or 1. The loss function calculates the squared difference on each pixel (k) of the frame and uses the c-matrix as a weighting for the pixels. For each pixel the c-matrix has a value 0 we tell the loss function that this pixel does not count on the temporal loss.

\subsubsection{Usage of optical flow}
\label{sec:usage_of_optical_flow}
In this implementation the optical flow is used in order to warp previously styled images, detecting disoccluded areas and motion boundaries. These areas are in turn used to create the c-matrix used in the loss function. this is done by giving the disoccluded areas and motion boundaries the value 0 in the c-matrix, and the rest of the image the value 1.\newline\newline To detect the disoccluded areas between two frames, we need the forward flow $(w)$, backward flow$(\hat{w})$, and the forward flow warped to the second image (\~{w}):  \newline \begin{equation}\~{w}(x, y) = w( (x, y) + \hat{w}(x, y))\end{equation}. \newline Using these functions the disoccluded areas are calculated using this inequality:\newline
\begin{equation}
|\~{w} + \hat{w}|^2 > 0.01(|\~{w}|^2 + |\hat{w}|^2) + 0.5
\end{equation}
When it comes to detecting motion boundaries this inequality is used: 
\begin{equation}
|\nabla\hat{u}|^2 + |\nabla\hat{v}|^2 > 0.01|\hat{w}|^2 + 0.002
\end{equation}
Where $\hat{u}$ and $\hat{v}$ are the two components of the backward flow function output: $\hat{w}(x, y) = (u, v)$

\subsubsection{long-term temporal loss}
\label{sec:long_term_temporal_loss}
In the first section, the temporal loss is described using only the previous frame, resulting in the total loss-function: 
\begin{equation}
\mathcal{L}_{shortterm}(p^{(i)}, a, x) = \alpha \mathcal{L}_{content} + \beta \mathcal{L}_{style} + \gamma \mathcal{L}_{temporal}(x^{(i)}, \omega_{i-1}^i(x^{i-1}, c^{i-1, i}))
\end{equation}
where the temporal loss is the function we described earlier, $x^{(i)}$ is the currently generated frame,  $\omega_{i-1}^i(x^{i-1}$ is the previous stylized image warped with the flow from previous frame to current frame, and $c^{i-1, i}$ is the weights of disoccluded areas and motion boundaries between last image and current image. However, the longterm loss function is described like this: \newline
\begin{equation}
\mathcal{L}_{longterm}(p^{(i)}, a, x) = \alpha \mathcal{L}_{content} + \beta \\\mathcal{L}_{style} + \gamma \sum_{j\in J: i-j \leq 1}\mathcal{L}_{temporal}(x^{(i)}, \omega_{i-j}^i(x^{i-j}), c_{long}^{i-j, i}))
\end{equation}
where e.g $J = [1, 2, 4]$. the most important part of this long-term loss function are the $c_{long}$ weights. they are calculated like this:\newline
\begin{equation}
c_{long}^{(i-j, i)} = max(c^{(i-j, i)} - \sum_{k\in J: i - k > i - j} c^{(i-k, i)}, 0)
\end{equation}
Here, max is taken element-wise\newline
\subsubsection{Multi-pass algorithm}
When generating videos with the new longterm temporal loss, the contrast is less diverse near image boundaries, and is especially noticable in videos with a lot of camera motion, mostly because the flow of information to base each frame on only comes from the first frame. This issue was solved by developing a multi-pass algorithm that initially stylize every frame individually and then blend frames with non-disoccluded parts of previous frames warped according to the optical flow, and the again rune some optimization steps for some iterations with the blend as an initialization.\newline
The initialization of frame i for pass j, when processed in forward direction is created like this:\newline
if i = 1: 
\begin{equation}
x'^{(i)(j)} = x^{(i)(j-1)}
\end{equation}
else:
\begin{equation}
x'^{(i)(j)} = \delta c^{i-1, i} \circ \omega_{i-1}^i(x^{(i-1)(j)}) + (\bar{\delta}1 + \delta \bar{c}^{(i-1, i)})\circ x^{(i)(j-1)}
\end{equation}\newline
when processed in backward direction, it's created like this:\newline
if i = $N_{frames}$:\newline
\begin{equation}
x'^{(i)(j)} = x^{(i)(j-1)}
\end{equation}
else:\newline
\begin{equation}
x'^{(i)(j)} = \delta c^{i+1, i} \circ \omega_{i+1}^i(x^{(i+1)(j)}) + (\bar{\delta}1 + \delta \bar{c}^{(i+1, i)})\circ x^{(i)(j-1)}
\end{equation}
