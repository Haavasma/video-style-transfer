\subsection{Loss functions}
Here we are going to discuss some of the loss functions that have been tested in this field so far, and that are relevant to this article. We will look away from the content loss as this part has stayed the same for virtually all papers on the matter.
\newline\newline
Style loss was initially discovered to be separable from content loss in a CNN (namely VGG16) by gatys et al \cite{Gatys:1}. They calculated what is known as the gram matrix which represents the style of an image as correlation between different features. The style loss is the squared error between the gram matrix of the features of the reference style image and the image for which we are minimizing the loss function. This function has been the basis for several other style transfer algorithms, and was for instance approximated with a neural net by Johnson et al \cite{Johnson:1} trained on a certain style.
\newline\newline
Another key finding was that the distribution of the features was a more complete representation of style. This lead to Huang et al. \cite{Huang:1} testing out minimizing the wasserstein distance between the features instead. This new representation of style produced results of higher artistic quality. Other techniques based the idea of the distance between two distributions as loss has also been tested out with MMD (Maximum mean discrepancy) as the distance measure in a paper by Li et al. \cite{Li:2} where they also combined it with the gram matrix. Chuan Li and Michael Wand used markov random fields to calculate an alternative to the gram matrix entirely \cite{Li:3}. Other style loss functions that noteworthy but not too relevant to delve into here are histogram loss \cite{Risser:1} and coral loss \cite{sun:1}
\newline\newline
Temporal loss has not been the main focus on this article we will just briefly go through the ones used. To go from picture style transfer to video style transfer we must introduce a temporal loss to avoid flickering, meaning that we should punish frames for being to different. The easiest way to do this is to just look at the difference between the current frame and the previous. This is known as short-term temporal loss. Long-term temporal loss is doing the same but for multiple previous frames. this will create more stable results given a greater context of the current frame.
\newline\newline
After applying style transfer on an image with Gatys et al \cite{Gatys:1}, we can end up with an image with many high frequency artifacts. To get rid of these variations in the output image, we want to penalize local variation. We can define this local variation loss as
\begin{equation}
    \sum{|y_{i+1}-y_i|^\beta}
\end{equation}
where $\beta\in\{1,2\}.$ Here $y$ are the pixel-values in one row of the output image \cite{Zhaoyou:1}. Now we can extend Equation (\ref{eq:total_loss}) with the total variation loss
\begin{equation}
    \mathcal{L}_\text{total}(\vec{p}, \vec{a}, \vec{x})=\alpha\mathcal{L}_\text{content}(\vec{p},\vec{x})+\beta\mathcal{L}_\text{style}(\vec{a},\vec{x})+\lambda\mathcal{L}_\text{TV}(\vec{x})
\end{equation}
where $\lambda$ is the weight for the total variation loss.
\newpage