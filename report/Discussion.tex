Gatys et al. \cite{Gatys:1} original paper used gradient descent to style a single image, as it was not intended for videos it had clear drawbacks when our naive implementation produced a video. Firstly, as each frame was rendered as an independent image, great flickering occurred. This was only in part due to noise, the loss function had no reason to prefer images that corresponded with the previous frame. So in the case of a video with a style with big brush strokes, the strokes would be regenerated and possible moved from frame to frame. Secondly, this technique was also quite slow, a gradient descent at this scale is very laborious if one wants longer videos with very high fps. The strength of this implementation is that it is easy to understand, and one may use whatever style image one wants to.\newline\newline
Adding the features described in Ruder et al. \cite{Ruder:1} paper greatly reduced the flickering. This is done by adding temporal loss in the long and short term. The advantage of this is that now we have a general style transfer algorithm that can reproduce any style. The price of this improved quality that now makes style transfer produce more reliable results is obvious during runtime. It usually takes a whole minute jsut to compute everything regarding deep-flow. This is in part because we are using numpy to calculate, which does not support running the calculations on the GPU.\newline\newline
The approach of Johnson et al. \cite{Johnson:1} is entirely different, and sacrifices flexibility of style for execution time. To try to make the algorithm faster, we replaced Batch normalization with Instance normalization as described in \cite{Ulyanov:1}. Having a neural net be trained on many images lets us estimate the style transfer function from Gatys et. al. \cite{Gatys:1} pretty well. As this has training data that is bound to not represent all possible styles it will not be prepared to produce good results for some input. This is not all that bad as it is still trained on a wide variety of data representing most artistic styles. In addition to this a frame in the video is now produced simply by feeding it to the network, compared to performing a gradient descent per image this is lightning fast (can run in realtime). Flickering was surprisingly not a problem in this program despite no implementation of temporal loss. This is probably due to the architecture of the program, a neural net will produce almost the same when given almost the same input (the next frame usually greatly resembles the previous one). This is not true for the gradient descent approach that can go in many different directions based on the starting point of the descent, and what the optimizer thinks is best.\newline\newline
The final implementation was based on the paper of Huang et al. \cite{Huang:1}. This paper aimed to define a more complete style loss that allowed for better transfer of the style. We managed to produce videos of higher artistic quality, at the great runtime expense of computing the Wasserstein distance. This technique could probably be improved without sacrificing to much runtime by estimating the Wasserstein distance in some way. \newline\newline
After all these implementations we combined Ruders paper \cite{Ruder:1} and Huangs paper \cite{Huang:1} to create our own algorithm for video style transfer. As Huangs technique produced the best artistic results of a single image we used the style transfer from this paper. Ruders technique managed to define temporal loss, which allows us to get more consistent and video-friendly results while using gradient descent. These two techniques merged together aims to maximize the quality of the result, and therefore paying heavily in terms of computation. If the purpose is some real-time application then having a neural net estimate the transfer would be much more fitting as the results are usually still very decent. 
\subsection{Further work}
We could also have explored the Adaptive Instance Normalization \cite{Huang:2} which aims to solve the problem of flexibility while being fast enough for real-time application. In addition to that, we could also have implemented a recurrent neural network to approximate the temporal loss with a network as well. We could also have introduced object detection in our architecture, so that we could style different objects in a frame independently to avoid objects melting together.
\newpage