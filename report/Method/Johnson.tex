\subsection{Styling with an image transformation network with instance normalization}
\label{seq:johnson_method}
This implementation differs from the two earlier implementations, since we trained a neural network to style the images, as described in Section \ref{sec:neuralnetwork}. The implementation is one of our own experiments, where we combine the architecture from Johnson et al \cite{Johnson:1} with the ideas from Ulyanov et al\cite{Ulyanov:1}. Our architecture was based on Figure \ref{fig:architecture}. We trained our image transformation network on the MS-COCO dataset \cite{Coco:1}, using stochastic gradient descent. The neural network consists of $5$ residual layers, where each residual layer is based on Figure \ref{fig:residual}. The convolutional layers are followed up by ReLU nonlinearities, except the last layer, which is followed up by a scaled $\tanh$, as discussed in Section \ref{sec:neuralnetwork}. To train the network faster, we replaced Batch normalization in the residual layers with Instance normalization, as presented by Ulyanov et al \cite{Ulyanov:1}. This was not done by Johnson et al.\newline\newline 
The loss network $\phi$ was implemented by using a pre-trained VGG19-network, unlike the VGG16-network used by Johnson et al \cite{Johnson:1}. The only difference between these two networks is the number of layers used in the network. We used the following layers in the VGG-19 network to extract the feature maps for the output image $\hat{y}$ from the image transformation network, and for $y_c$ and $y_s$, the content and style images. The perceptual loss functions used to calculate the style loss and content loss, are discussed in Section \ref{sec:loss}.\newline\newline
To conclude, our architecture was based on Figure $\ref{fig:architecture}$, with the exception that we used instance normalization instead of batch normalization, and used a different loss network $\phi,$ namely VGG-19. As discussed in Section \ref{seq:johnson}, we concluded that it is faster to style an image with this method because we only have to do one forward-pass through the image transformation network to get the styled image. Combining this method with instance normalization made the stylization of the images faster, as discussed in Section \ref{sec:instance_normalization}. This speed-up let us produce videos in $30$ frames per second with a maximum dimension of 1024. We fed the images to the network frame for frame, and we did not use temporal loss as discussed in Section \ref{seq:ruder}. Because we used a neural network to solve the optimization problem, the flickering on the video became surprisingly low. This is because the trained image transformation network is parameterized by a distinct set of weights $W,$ and these do not change during of the frames. So two consecutive frames $x_i$ and $x_{i+1}$ will yield almost the same stylized images $\hat{y}_i$ and $\hat{y}_{i+1},$ resulting in lower flickering and a smoother video. The results from this implementation can be viewed in Section \ref{seq:johnson_result}.