\subsection{BN Matching}
The traditional method of implementing style transfer is using the mean square error (MSE) of the Gramian Matrix of the feature maps. This the the method that is used in the original Gatys style transfer and is also the basis in Ruder. But there are other such methods, one of which is BN Matching which was first implemented by Li \textit{et al.} \cite{Li:1}.\newline\newline
BN (batch normalization) Matching is based on matching the mean and standard deviation of the features. This type of styling uses 1st order statistics, unlike traditional style transfer which uses 2nd order statistics. The advantage of using 1st order statistics is that it requires less computation and therefore is faster. For that reason it is often utilized for commercial purposes. The downside is that it sacrifices quality for speed.\newline\newline
The original purpose of BN normalization is to reduce the issue of internal covariate shifting, which is a common problem with training deep neural networks. The normalization works by standardizing each feature into mini-batches and then it learns a common slope and bias for each mini-batch. The BN layer makes transformations on our model for each feature $j$ in the following manner.
\begin{equation}
\begin{aligned}
\label{eq:batch_normalization}
    \hat{x}_j&=\frac{x_j-{\mathbb{E}}[X_{.j}]}{\sqrt{Var[X_{.j}]}}\\
    y_j&=\gamma_j\hat{x}_j + \beta_j
\end{aligned}
\end{equation}
Here $x_j$ and $y_j$ are the input and output scalars of one neuron in our data, $X_{.j}$ is the $j^{th}$ column of the input, while $\gamma_j$ and $\beta_j$ are to be learned. Using this method we can calculate the batch normalization for each feature. The core idea of this is to align the distribution of our data.  With this we can use BN statistics to represent the style of our image and we can construct a loss function by matching the mean and standard deviation of the features. 


